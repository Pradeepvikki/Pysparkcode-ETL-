# Pysparkcode-ETL-
‚úÖ PySpark Notebook ‚Äì End-to-End Workflow (GitHub Points)
1Ô∏è‚É£ Start Spark Session

Initialized a SparkSession in the PySpark notebook.

Configured required settings for local execution.

2Ô∏è‚É£ Load Dataset

Loaded the dataset using:

spark.read.csv()

Options like header=True, inferSchema=True

Verified successful loading with printSchema() and .show().

3Ô∏è‚É£ Data Display & Exploration

Displayed records using:

.show()

.limit()

.take()

.head()

Explored schema, data types, and record count:

.printSchema()

.columns

.count()

4Ô∏è‚É£ Basic DataFrame Operations

Selected specific columns using .select().

Renamed columns using .withColumnRenamed().

Dropped unwanted columns using .drop().

Removed duplicate rows using .dropDuplicates().

5Ô∏è‚É£ Data Cleaning & Transformation

Standardized column values using:

.withColumn()

trim(), lower(), upper()

regexp_replace()

Cleaned incorrect values such as:

Wrong date formats

Misspelled statuses

Invalid emails

Numeric columns in string/word format

6Ô∏è‚É£ Handling Missing Values

Used:

dropna() to remove NULL rows

fillna() to fill missing values

Converted invalid entries (e.g., NaN, words, symbols) to NULL.

7Ô∏è‚É£ Type Casting

Casted columns to correct data types:

cast(IntegerType())

cast(FloatType())

to_date() for date columns

Ensured numeric columns contain only numeric values.

8Ô∏è‚É£ Column Standardization

Standardized formats:

Customer IDs ‚Üí C001, C002, etc.

Delivery Status ‚Üí Delivered, Pending, etc.

Payment Mode ‚Üí Credit Card, Debit Card, UPI

Rounded amounts to 2 decimals.

9Ô∏è‚É£ Full Dataset Cleanup

Applied a full cleaning pipeline:

Date normalization

Email validation

Removing special characters

Converting word-numbers to digits

Ensuring city names are valid

üîü Final Output

Displayed cleaned DataFrame using .show().

Saved cleaned data as:

CSV

Parquet

JSON
